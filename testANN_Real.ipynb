{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591878ec",
   "metadata": {},
   "source": [
    "# 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff0f1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, hashlib, html, string, pandas as pd, numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "RANDOM_SEED = 42 \n",
    "np.random.seed(RANDOM_SEED) \n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac6f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = \"dataset/IMDB Dataset.csv\" # Kaggle file \n",
    "OUT_PATH = \"dataset/imdb_clean_split.csv\" # Shared artifact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75171487",
   "metadata": {},
   "source": [
    "# 2. Clean Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55177d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    # Unescape & strip HTML\n",
    "    x = html.unescape(x)\n",
    "    x = BeautifulSoup(x, \"lxml\").get_text(separator=\" \")\n",
    "\n",
    "    # Unicode normalize + unify curly quotes to straight ones\n",
    "    x = unicodedata.normalize(\"NFKC\", x)\n",
    "    x = x.replace(\"“\", \"'\").replace(\"”\", \"'\").replace(\"‘\", \"'\").replace(\"’\", \"'\").replace('\"', \"'\")\n",
    "\n",
    "    # Neutralize obvious artifacts\n",
    "    x = re.sub(r\"(https?://\\S+)|(\\w+\\.\\w+/\\S+)\", \" \", x)\n",
    "    x = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", x)\n",
    "\n",
    "    # runs of 2+ asterisks → single *\n",
    "    x = re.sub(r\"\\{2,}\", \"\", x)\n",
    "\n",
    "    # collapse any run of -, – or — to a single em dash, with spacing\n",
    "    x = re.sub(r\"\\s*[-–—]{2,}\\s*\", \" — \", x)\n",
    "\n",
    "    # \"\" → \"   and   '' → '\n",
    "    x = re.sub(r'([\\'\\\"])\\1+', r'\\1', x)  # collapse immediate repeats\n",
    "    # also clean cases with whitespace between repeated quotes: \"  \" → \"\n",
    "    x = re.sub(r'([\\'\"])\\s+\\1', r'\\1', x)\n",
    "\n",
    "    # cap !!!!! or ????? at two; dots at an ellipsis\n",
    "    x = re.sub(r\"([!?])\\1{2,}\", r\"\\1\\1\", x)   # keep at most two\n",
    "    x = re.sub(r\"\\.{3,}\", \"…\", x)\n",
    "\n",
    "    # 5) Remove control chars & collapse whitespace\n",
    "    x = re.sub(r\"[\\u0000-\\u001F\\u007F]\", \" \", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb606ca",
   "metadata": {},
   "source": [
    "# 3. Read Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7709905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(IN_PATH)  # columns: review, sentiment\n",
    "df[\"review_clean\"] = df[\"review\"].apply(clean_text)\n",
    "df[\"label\"] = (df[\"sentiment\"].str.lower() == \"positive\").astype(int)\n",
    "\n",
    "# drop null/empty data first\n",
    "df = df.dropna(subset=['review_clean']).copy()  # drop rows where review_clean is null\n",
    "# deduplicate (after cleaning) - keep first occurrence\n",
    "def row_key(s): return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "df[\"dup_key\"] = df[\"review_clean\"].apply(row_key)\n",
    "df = df.drop_duplicates(subset=[\"dup_key\"], keep='first').drop(columns=[\"dup_key\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb8aea",
   "metadata": {},
   "source": [
    "# 4. Train/Validate/Test Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57935971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    34704\n",
      "test      9916\n",
      "val       4958\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    24882\n",
      "0    24696\n",
      "Name: count, dtype: int64\n",
      "            n_chars       n_words\n",
      "count  49578.000000  49578.000000\n",
      "mean    1286.624249    229.274013\n",
      "std      972.995822    169.952820\n",
      "min       32.000000      4.000000\n",
      "50%      954.000000    172.000000\n",
      "75%     1560.000000    278.000000\n",
      "90%     2532.000000    448.000000\n",
      "95%     3334.000000    585.150000\n",
      "99%     5100.460000    898.230000\n",
      "max    13593.000000   2459.000000\n",
      "Saved: dataset/imdb_clean_split.csv\n"
     ]
    }
   ],
   "source": [
    "X = df[\"review_clean\"].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: from the 80%, split into 70% train (87.5% of temp) and 10% val (12.5% of temp)\n",
    "# 0.125 = 10% / 80% (to get 10% of original data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.125, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Create split column\n",
    "df[\"split\"] = \"train\"  # default\n",
    "df.loc[df[\"review_clean\"].isin(X_test), \"split\"] = \"test\"\n",
    "df.loc[df[\"review_clean\"].isin(X_val), \"split\"] = \"val\"\n",
    "\n",
    "# basic EDA\n",
    "print(df[\"split\"].value_counts())\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "df[\"n_chars\"] = df[\"review_clean\"].str.len()\n",
    "df[\"n_words\"] = df[\"review_clean\"].str.split().apply(len)\n",
    "print(df[[\"n_chars\",\"n_words\"]].describe(percentiles=[.5,.75,.9,.95,.99]))\n",
    "\n",
    "df = df.rename(columns={\"review_clean\":\"text\"})\n",
    "df[[\"text\",\"label\",\"split\"]].to_csv(OUT_PATH, index=False)\n",
    "print(\"Saved:\",OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09f620",
   "metadata": {},
   "source": [
    "# 5. TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29b5b7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features: 30000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle, math, os \n",
    "\n",
    "# ----- TF-IDF (fit on train only) ----- \n",
    "tfidf = TfidfVectorizer( \n",
    "    max_features=30000, # keep consistent across models for fair compare \n",
    "    ngram_range=(1, 2), \n",
    "    min_df=2, \n",
    "    max_df=0.95, \n",
    "    sublinear_tf=True, \n",
    "    lowercase=True, \n",
    "    strip_accents=\"unicode\"\n",
    ") \n",
    "tfidf.fit(X_train) \n",
    "\n",
    "X_train_tf = tfidf.transform(X_train).astype(np.float32) # sparse CSR \n",
    "X_val_tf = tfidf.transform(X_val).astype(np.float32) \n",
    "X_test_tf = tfidf.transform(X_test).astype(np.float32) \n",
    "N_FEATS = X_train_tf.shape[1] \n",
    "print(\"TF-IDF features:\", N_FEATS) \n",
    "\n",
    "# ----- Sparse→dense per-batch generator (saves RAM) ----- \n",
    "class CSRBatchGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X_csr, y=None, batch_size=256, shuffle=True, **kwargs):\n",
    "        # Keras 3 wants this so fit(workers=..., use_multiprocessing=...) can flow in.\n",
    "        super().__init__(**kwargs)\n",
    "        self.X = X_csr.astype(np.float32)\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.idx = np.arange(self.X.shape[0])\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        import math\n",
    "        return math.ceil(self.X.shape[0] / self.bs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sl = slice(i*self.bs, (i+1)*self.bs)\n",
    "        ii = self.idx[sl]\n",
    "        Xb = self.X[ii].toarray()  # dense only for this batch\n",
    "        if self.y is None:\n",
    "            return Xb\n",
    "        return Xb, self.y[ii]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fb8b1",
   "metadata": {},
   "source": [
    "# 6. Build the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b09c78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,840,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,257</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,840,257\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,257</span> (14.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,840,257\u001b[0m (14.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = layers.Input(shape=(N_FEATS,), dtype=\"float32\")\n",
    "x = layers.Dense(128, activation=\"relu\",\n",
    "                 kernel_regularizer=regularizers.l2(1e-4))(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(3e-4),\n",
    "              loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2c920",
   "metadata": {},
   "source": [
    "# 7. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded2121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 75ms/step - accuracy: 0.8282 - loss: 0.6233 - val_accuracy: 0.8737 - val_loss: 0.5256\n",
      "Epoch 2/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - accuracy: 0.8913 - loss: 0.4583 - val_accuracy: 0.8923 - val_loss: 0.4150\n",
      "Epoch 3/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.9085 - loss: 0.3782 - val_accuracy: 0.9006 - val_loss: 0.3694\n",
      "Epoch 4/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.9206 - loss: 0.3381 - val_accuracy: 0.9056 - val_loss: 0.3473\n",
      "Epoch 5/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 73ms/step - accuracy: 0.9283 - loss: 0.3126 - val_accuracy: 0.9066 - val_loss: 0.3346\n",
      "Epoch 6/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 71ms/step - accuracy: 0.9354 - loss: 0.2962 - val_accuracy: 0.9086 - val_loss: 0.3261\n",
      "Epoch 7/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.9391 - loss: 0.2824 - val_accuracy: 0.9090 - val_loss: 0.3202\n",
      "Epoch 8/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - accuracy: 0.9446 - loss: 0.2705 - val_accuracy: 0.9106 - val_loss: 0.3158\n",
      "Epoch 9/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 72ms/step - accuracy: 0.9495 - loss: 0.2602 - val_accuracy: 0.9096 - val_loss: 0.3126\n",
      "Epoch 10/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - accuracy: 0.9529 - loss: 0.2516 - val_accuracy: 0.9086 - val_loss: 0.3102\n",
      "Epoch 11/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 79ms/step - accuracy: 0.9577 - loss: 0.2437 - val_accuracy: 0.9090 - val_loss: 0.3081\n",
      "Epoch 12/12\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - accuracy: 0.9583 - loss: 0.2365 - val_accuracy: 0.9094 - val_loss: 0.3062\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS     = 12\n",
    "\n",
    "train_gen = CSRBatchGenerator(X_train_tf, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_gen   = CSRBatchGenerator(X_val_tf,   y_val,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, mode=\"max\", restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1),\n",
    "]\n",
    "\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e984dc3",
   "metadata": {},
   "source": [
    "# 8. Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184d0016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
      "\n",
      "=== Final Test Metrics (Weighted) ===\n",
      "Accuracy        : 0.9077\n",
      "Precision (Wgt) : 0.9078\n",
      "Recall (Wgt)    : 0.9077\n",
      "F1 (Weighted)   : 0.9077\n",
      "\n",
      "Classification report (includes weighted avg row):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.9115    0.9024    0.9069      4939\n",
      "    positive     0.9041    0.9130    0.9085      4977\n",
      "\n",
      "    accuracy                         0.9077      9916\n",
      "   macro avg     0.9078    0.9077    0.9077      9916\n",
      "weighted avg     0.9078    0.9077    0.9077      9916\n",
      "\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      "[[4457  482]\n",
      " [ 433 4544]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "test_gen = CSRBatchGenerator(X_test_tf, batch_size=512, shuffle=False)\n",
    "proba = model.predict(test_gen).ravel()\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Test Metrics (Weighted) ===\")\n",
    "print(f\"Accuracy        : {acc:.4f}\")\n",
    "print(f\"Precision (Wgt) : {prec_w:.4f}\")\n",
    "print(f\"Recall (Wgt)    : {rec_w:.4f}\")\n",
    "print(f\"F1 (Weighted)   : {f1_w:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (includes weighted avg row):\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"negative\",\"positive\"], digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786ed85",
   "metadata": {},
   "source": [
    "#  9. Save model & vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba82e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/ann/imdb_mlp_tfidf.keras\")\n",
    "with open(\"models/ann/tfidf.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544236b",
   "metadata": {},
   "source": [
    "# 10. Predict the Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070986ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('negative', 0.018184518441557884)\n",
      "('negative', 0.2542766034603119)\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text: str):\n",
    "    text = clean_text(text)\n",
    "    Xv = tfidf.transform([text]).astype(np.float32).toarray()\n",
    "    p  = model.predict(Xv, verbose=0)[0, 0]\n",
    "    label = \"positive\" if p >= 0.5 else \"negative\"\n",
    "    return label, float(p)\n",
    "\n",
    "print(predict_sentiment('\"Fifty Shades of Grey\" was painful to sit through. The chemistry between the leads was practically nonexistent, and the dialogue often felt laughable. What was supposed to be a story about passion and complexity ended up being dull and awkward. The pacing dragged, and scenes that were meant to be intense came across as cringe-worthy. It wasn’t romantic, it wasn’t sexy, and it wasn’t dramatic—it was just boring.'))\n",
    "print(predict_sentiment('\"Transformers: The Last Knight\" was exhausting. It felt less like a movie and more like a two-and-a-half-hour commercial for explosions and CGI robots.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa2edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "tfidf.pkl:   0%|          | 0.00/1.16M [00:00<?, ?B/s]\n",
      "tfidf.pkl:  38%|███▊      | 442k/1.16M [00:00<00:00, 4.16MB/s]\n",
      "tfidf.pkl:  98%|█████████▊| 1.13M/1.16M [00:00<00:00, 4.70MB/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "tfidf.pkl: 100%|██████████| 1.16M/1.16M [00:02<00:00, 508kB/s] \n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "imdb_mlp_tfidf.keras: 100%|██████████| 46.1M/46.1M [00:09<00:00, 5.12MB/s]\n",
      "Upload 2 LFS files: 100%|██████████| 2/2 [00:09<00:00,  4.75s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/noobrs/ann-movie-sentiment/commit/4136ecd4e05b8b8f166437273065159dc40f6799', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4136ecd4e05b8b8f166437273065159dc40f6799', pr_url=None, repo_url=RepoUrl('https://huggingface.co/noobrs/ann-movie-sentiment', endpoint='https://huggingface.co', repo_type='model', repo_id='noobrs/ann-movie-sentiment'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=r\"D:\\ai-app\\models\\ann\",\n",
    "    repo_id=\"noobrs/ann-movie-sentiment\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28bcbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('negative', 0.018184518441557884)\n",
      "('negative', 0.2542766034603119)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "SIDE_REPO = \"noobrs/ann-movie-sentiment\"  # your repo\n",
    "\n",
    "def load_ann():\n",
    "    try:\n",
    "        tfidf_p = hf_hub_download(SIDE_REPO, filename=\"tfidf.pkl\")\n",
    "        ann_p   = hf_hub_download(SIDE_REPO, filename=\"imdb_mlp_tfidf.keras\")\n",
    "        import tensorflow as tf, joblib\n",
    "        return {\"model\": tf.keras.models.load_model(ann_p),\n",
    "                \"tfidf\": joblib.load(tfidf_p)}\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "def predict_sentiment(text: str):\n",
    "    model_data = load_ann()\n",
    "    if model_data is None:\n",
    "        return \"error\", 0.0\n",
    "    model = model_data[\"model\"]\n",
    "    tfidf = model_data[\"tfidf\"]\n",
    "\n",
    "    text = clean_text(text)\n",
    "    Xv = tfidf.transform([text]).astype(np.float32).toarray()\n",
    "    p  = model.predict(Xv, verbose=0)[0, 0]\n",
    "    label = \"positive\" if p >= 0.5 else \"negative\"\n",
    "    return label, float(p)\n",
    "\n",
    "print(predict_sentiment('\"Fifty Shades of Grey\" was painful to sit through. The chemistry between the leads was practically nonexistent, and the dialogue often felt laughable. What was supposed to be a story about passion and complexity ended up being dull and awkward. The pacing dragged, and scenes that were meant to be intense came across as cringe-worthy. It wasn’t romantic, it wasn’t sexy, and it wasn’t dramatic—it was just boring.'))\n",
    "print(predict_sentiment('\"Transformers: The Last Knight\" was exhausting. It felt less like a movie and more like a two-and-a-half-hour commercial for explosions and CGI robots.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
